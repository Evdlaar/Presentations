{
    "metadata": {
        "kernelspec": {
            "name": "pysparkkernel",
            "display_name": "PySpark"
        },
        "language_info": {
            "name": "pyspark",
            "mimetype": "text/x-python",
            "codemirror_mode": {
                "name": "python",
                "version": 2
            },
            "pygments_lexer": "python2"
        }
    },
    "nbformat_minor": 2,
    "nbformat": 4,
    "cells": [
        {
            "cell_type": "markdown",
            "source": [
                "# Querying with Spark\n",
                "Since we have both SQL Server access as well as Spark installed on the Big Data Cluster nodes, we can also run Spark commands against files on the HDFS filesystem, or even against the SQL Master Instance"
            ],
            "metadata": {
                "azdata_cell_guid": "5ca596ec-f3bf-4089-acb7-275837ca351a"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "# Import the csv files from HDFS\n",
                "df_airports = spark.read.format('csv').options(header='true', inferSchema='true').load('/Flight_Delays/airports.csv')\n",
                "df_airlines = spark.read.format('csv').options(header='true', inferSchema='true').load('/Flight_Delays/airlines.csv')\n",
                "df_flights = spark.read.format('csv').options(header='true', inferSchema='true').load('/Flight_Delays/flights.csv')"
            ],
            "metadata": {
                "azdata_cell_guid": "99bf3fac-9ea4-4859-bdaf-c0920de12632"
            },
            "outputs": [],
            "execution_count": 3
        },
        {
            "cell_type": "code",
            "source": [
                "# Let's take a look at some of the dataframe's content\n",
                "df_airports.show(10)"
            ],
            "metadata": {
                "azdata_cell_guid": "9412672e-2686-49ba-a757-237cf16e8219"
            },
            "outputs": [],
            "execution_count": 4
        },
        {
            "cell_type": "code",
            "source": [
                "# Let's join some data here as well\n",
                "from pyspark.sql.functions import *\n",
                "\n",
                "# We will join both dataframes again but this time drop the AIRLINE column of the df_flights dataframe\n",
                "df_flightinfo = df_flights.join(df_airlines, df_flights.AIRLINE == df_airlines.IATA_CODE, how=\"inner\").drop(df_flights.AIRLINE)\n",
                "\n",
                "# Select a number of columns from the joined dataframe\n",
                "df_flightinfo.select(\"FLIGHT_NUMBER\", \"AIRLINE\", \"SCHEDULED_TIME\", \"ELAPSED_TIME\").show()"
            ],
            "metadata": {
                "azdata_cell_guid": "6e98817b-7794-472e-830f-d216fbc4ba8e"
            },
            "outputs": [],
            "execution_count": 6
        },
        {
            "cell_type": "code",
            "source": [
                "# We are not limited to the PySpark command however. Spark allows us to write our trusted SQL as well!\n",
                "# To do this we have to register the df_flightinfo dataframe as a (temporary) table so we can run SQL queries against it\n",
                "df_flightinfo.registerTempTable(\"FlightInfoTable\")\n",
                ""
            ],
            "metadata": {
                "azdata_cell_guid": "4689c34c-044d-47c7-82ce-895f547b0308"
            },
            "outputs": [],
            "execution_count": 7
        },
        {
            "cell_type": "code",
            "source": [
                "# Select some rows useing a SQL statement\n",
                "sqlContext.sql(\"SELECT FLIGHT_NUMBER, ORIGIN_AIRPORT, DESTINATION_AIRPORT, ELAPSED_TIME FROM FlightInfoTable\").show(10)"
            ],
            "metadata": {
                "azdata_cell_guid": "7ad1b34e-e4bb-458f-a0e2-0068df00c8e2"
            },
            "outputs": [],
            "execution_count": 8
        },
        {
            "cell_type": "code",
            "source": [
                "# Some more advanced SQL use:\n",
                "# Group the flight distance for each airline and return the average flight distance for each flight\n",
                "sqlContext.sql(\"SELECT AIRLINE, AVG(DISTANCE) FROM FlightInfoTable GROUP BY AIRLINE ORDER BY 'avg(Distance)' DESC\").show()"
            ],
            "metadata": {
                "azdata_cell_guid": "bcfdc08a-ee2b-4833-b105-b941c1d49217"
            },
            "outputs": [],
            "execution_count": 9
        },
        {
            "cell_type": "code",
            "source": [
                "# Your Data Engineers and Scientists want to get something from the master instance?\n",
                "# No problemo! Just read a table into a dataframe!\n",
                "df_sqldb_sales = spark.read.format(\"jdbc\") \\\n",
                "    .option(\"url\", \"jdbc:sqlserver://master-0.master-svc;databaseName=AdventureWorks\") \\\n",
                "    .option(\"dbtable\", \"Sales.SalesOrderDetail\") \\\n",
                "    .option(\"user\", \"[your username]\") \\\n",
                "    .option(\"password\", \"[your password]\").load()"
            ],
            "metadata": {
                "azdata_cell_guid": "af35de13-497b-4caf-a3d0-7e269eb01dcf"
            },
            "outputs": [],
            "execution_count": 13
        },
        {
            "cell_type": "code",
            "source": [
                "df_sqldb_sales.show(10)"
            ],
            "metadata": {
                "azdata_cell_guid": "d46baead-a918-4bf6-9047-b6dde09b771e"
            },
            "outputs": [],
            "execution_count": 14
        },
        {
            "cell_type": "code",
            "source": [
                "%matplotlib inline \n",
                "\n",
                "# We can do more cool stuff, like plotting\n",
                "# if the command fails, install matplotlib through Manage Packages\n",
                "import matplotlib\n",
                "\n",
                "# We are using pandas to display graphs, pandas has some built-in graph functions\n",
                "# Also we require to have our dataframe in-memory of the Spark master we are running\n",
                "# this code on\n",
                "import pandas as pd\n",
                "\n",
                "# Create a local pandas dataframe from a csv through an URL\n",
                "pd_dataframe = pd.read_csv(\"https://github.com/Evdlaar/Presentations/raw/master/Advanced%20Analytics%20in%20the%20Cloud/automobiles.csv\")\n",
                ""
            ],
            "metadata": {
                "azdata_cell_guid": "569895c3-e7cf-4863-b438-429831951fd6"
            },
            "outputs": [],
            "execution_count": 6
        },
        {
            "cell_type": "code",
            "source": [
                "%matplotlib inline \n",
                "\n",
                "# The first line above is very important\n",
                "# If we do not include it the graphs are not shown inside the notebook\n",
                "# and various errors will be returned\n",
                "\n",
                "# We can create a histogram, for instance for the horsepower column\n",
                "pd_dataframe.hist(\"horsepower\")"
            ],
            "metadata": {
                "azdata_cell_guid": "fabc6d2d-4811-40fb-b7fa-99d66f902b67"
            },
            "outputs": [],
            "execution_count": 7
        }
    ]
}